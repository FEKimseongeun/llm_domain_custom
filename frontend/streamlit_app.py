import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from pathlib import Path
import json
import re
from collections import defaultdict
from typing import List, Dict, Any
import tempfile
import os

# ì„¤ì •
st.set_page_config(
    page_title="Industrial RAG Assistant",
    page_icon="ğŸ­",
    layout="wide"
)


@st.cache_resource
def load_llm_model():
    """ê²½ëŸ‰ LLM ëª¨ë¸ ë¡œë“œ (ìºì‹œë¨)"""
    try:
        # ë§¤ìš° ê°€ë²¼ìš´ ëª¨ë¸ (Streamlit Cloud í˜¸í™˜)
        model_name = "distilgpt2"  # ì•½ 300MBë§Œ ì‚¬ìš©

        with st.spinner("Loading AI model... (first time only)"):
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)

            # íŒ¨ë”© í† í° ì„¤ì •
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
            generator = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=0 if torch.cuda.is_available() else -1,  # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )

            return generator
    except Exception as e:
        st.error(f"Model loading failed: {e}")
        return None


@st.cache_resource
def load_simple_model():
    """ë” ê°„ë‹¨í•œ ëª¨ë¸ (fallback)"""
    try:
        # ë§¤ìš° ê°€ë²¼ìš´ ëª¨ë¸
        return pipeline("text-generation", model="distilgpt2", device=-1)
    except:
        return None


class SimpleDocumentStore:
    """ê°„ë‹¨í•œ ë¬¸ì„œ ì €ì¥ ë° ê²€ìƒ‰"""

    def __init__(self, category: str):
        self.category = category
        self.documents = []
        self.keyword_index = defaultdict(set)

        # ì„¸ì…˜ ìƒíƒœì—ì„œ ë¡œë“œ
        if f"docs_{category}" in st.session_state:
            self.documents = st.session_state[f"docs_{category}"]
            self._rebuild_index()

    def _rebuild_index(self):
        """í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì¬êµ¬ì„±"""
        self.keyword_index = defaultdict(set)
        for i, doc in enumerate(self.documents):
            self._index_document(doc['text'], i)

    def _index_document(self, text: str, doc_id: int):
        """ë¬¸ì„œë¥¼ í‚¤ì›Œë“œë¡œ ì¸ë±ì‹±"""
        words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
        for word in set(words):
            self.keyword_index[word].add(doc_id)

    def add_document(self, text: str, metadata: dict):
        """ë¬¸ì„œ ì¶”ê°€"""
        doc_id = len(self.documents)
        self.documents.append({
            'text': text,
            'metadata': metadata
        })

        # í‚¤ì›Œë“œ ì¸ë±ì‹±
        self._index_document(text, doc_id)

        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state[f"docs_{self.category}"] = self.documents

    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        if not self.documents:
            return []

        query_words = re.findall(r'\b[a-zA-Z]{3,}\b', query.lower())
        if not query_words:
            return []

        # ë¬¸ì„œë³„ ì ìˆ˜ ê³„ì‚°
        doc_scores = defaultdict(int)
        for word in query_words:
            for doc_id in self.keyword_index.get(word, set()):
                doc_scores[doc_id] += 1

        # ì ìˆ˜ ìˆœ ì •ë ¬
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)

        # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        results = []
        for doc_id, score in sorted_docs[:top_k]:
            if score > 0:
                doc = self.documents[doc_id]
                results.append({
                    'text': doc['text'],
                    'metadata': doc['metadata'],
                    'score': score / len(query_words)
                })

        return results

    def get_document_count(self) -> int:
        return len(self.documents)


def extract_text_from_file(uploaded_file) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        if uploaded_file.type == "text/plain":
            return str(uploaded_file.read(), "utf-8")
        elif uploaded_file.type == "application/pdf":
            try:
                import PyPDF2
                reader = PyPDF2.PdfReader(uploaded_file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()
                return text
            except:
                return "PDF processing not available. Please upload as text file."
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„
            return str(uploaded_file.read(), "utf-8")
    except Exception as e:
        return f"Error reading file: {e}"


def generate_answer_with_local_llm(question: str, context: str, category: str, generator) -> str:
    """ë¡œì»¬ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    if not generator:
        return "AI model not available. Please check the setup."

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì§§ê²Œ ìœ ì§€)
    prompt = f"""Context: {context[:500]}...

Question: {question}

Answer based on the context above:"""

    try:
        # í…ìŠ¤íŠ¸ ìƒì„±
        result = generator(
            prompt,
            max_length=len(prompt.split()) + 100,  # ì…ë ¥ + 100 í† í°
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=generator.tokenizer.eos_token_id
        )

        # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        generated_text = result[0]['generated_text']
        answer = generated_text[len(prompt):].strip()

        if not answer:
            return "Based on the documents, I found relevant information but couldn't generate a complete answer. Please try rephrasing your question."

        return answer

    except Exception as e:
        return f"Error generating answer: {e}"


def generate_simple_answer(question: str, context: str) -> str:
    """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë‹µë³€ (fallback)"""
    # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê°„ë‹¨í•œ ë‹µë³€ ìƒì„±
    sentences = context.split('.')[:3]  # ì²˜ìŒ 3ë¬¸ì¥
    answer = ". ".join(sentences) + "."
    return f"Based on the documents: {answer}"


def main():
    st.title("ğŸ­ Industrial Document Assistant")
    st.markdown("**Free AI-powered document search** (no API costs!)")

    # AI ëª¨ë¸ ë¡œë“œ
    with st.sidebar:
        st.header("ğŸ¤– AI Model Status")

        # ëª¨ë¸ ì„ íƒ
        model_option = st.selectbox(
            "AI Model:",
            ["Local AI (Free)", "Simple Search Only"]
        )

        if model_option == "Local AI (Free)":
            generator = load_llm_model()
            if not generator:
                generator = load_simple_model()
                if generator:
                    st.warning("Using basic model")
                else:
                    st.error("AI model failed to load")
        else:
            generator = None
            st.info("Using keyword search only")

        st.markdown("---")

        # ì¹´í…Œê³ ë¦¬ ì„ íƒ
        categories = {
            "procurement": "Procurement/Contract",
            "piping": "Piping",
            "process": "Process",
            "mechanical": "Mechanical"
        }

        selected_category = st.selectbox(
            "Document Category:",
            options=list(categories.keys()),
            format_func=lambda x: categories[x]
        )

        st.markdown("---")

        # ë¬¸ì„œ ì—…ë¡œë“œ
        st.header("ğŸ“¤ Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload files",
            accept_multiple_files=True,
            type=['txt', 'md'],
            help="Supported: TXT, MD files"
        )

        if uploaded_files and st.button("Process Documents"):
            store = SimpleDocumentStore(selected_category)

            progress_bar = st.progress(0)
            processed = 0

            for i, uploaded_file in enumerate(uploaded_files):
                text = extract_text_from_file(uploaded_file)
                if text.strip() and len(text) > 10:
                    metadata = {
                        "category": selected_category,
                        "file_name": uploaded_file.name,
                        "file_type": uploaded_file.type
                    }
                    store.add_document(text, metadata)
                    processed += 1

                progress_bar.progress((i + 1) / len(uploaded_files))

            st.success(f"âœ… Processed {processed} documents!")
            st.rerun()

        # í†µê³„ í‘œì‹œ
        st.header("ğŸ“Š Document Stats")
        for category, name in categories.items():
            if f"docs_{category}" in st.session_state:
                count = len(st.session_state[f"docs_{category}"])
                if count > 0:
                    st.metric(name, count)

    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.header("ğŸ’¬ Chat with Documents")

    # ë„ì›€ë§
    with st.expander("â„¹ï¸ How it works", expanded=False):
        st.markdown("""
        **This app runs completely free!**

        - **No API costs**: Uses local AI models
        - **Private**: Your documents stay on your device
        - **Simple**: Upload documents and ask questions

        **Tips:**
        - Upload text files for best results
        - Ask specific questions about your documents
        - The AI model loads once and stays cached
        """)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            if message["role"] == "assistant" and "sources" in message:
                if message["sources"]:
                    with st.expander("ğŸ“„ Sources", expanded=False):
                        for i, source in enumerate(message["sources"], 1):
                            st.markdown(f"**{i}. {source['file_name']}** (Score: {source['score']:.3f})")
                            st.markdown(f"*{source['preview']}*")

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("Ask about your documents..."):
        # ë¬¸ì„œ í™•ì¸
        store = SimpleDocumentStore(selected_category)
        if store.get_document_count() == 0:
            st.error(f"No documents in {categories[selected_category]} category. Please upload documents first.")
            return

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
        with st.chat_message("assistant"):
            with st.spinner("Searching documents..."):
                # ë¬¸ì„œ ê²€ìƒ‰
                search_results = store.search(prompt, top_k=3)

                if not search_results:
                    response = f"No relevant documents found in {categories[selected_category]} category."
                    sources = []
                else:
                    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    context_texts = []
                    sources = []

                    for result in search_results:
                        text = result['text'][:600]  # í† í° ì œí•œìœ¼ë¡œ ì§§ê²Œ
                        context_texts.append(text)
                        sources.append({
                            'file_name': result['metadata']['file_name'],
                            'score': result['score'],
                            'preview': text[:200] + "..." if len(text) > 200 else text
                        })

                    context = "\n\n---\n\n".join(context_texts)

                    # AI ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
                    if generator and model_option == "Local AI (Free)":
                        response = generate_answer_with_local_llm(prompt, context, categories[selected_category],
                                                                  generator)
                    else:
                        response = generate_simple_answer(prompt, context)

                st.markdown(response)

                # ì†ŒìŠ¤ í‘œì‹œ
                if sources:
                    with st.expander("ğŸ“„ Sources", expanded=False):
                        for i, source in enumerate(sources, 1):
                            st.markdown(f"**{i}. {source['file_name']}** (Score: {source['score']:.3f})")
                            st.markdown(f"*{source['preview']}*")

                # ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "sources": sources
                })

    # ì±„íŒ… ì´ˆê¸°í™”
    if st.session_state.messages:
        if st.button("ğŸ—‘ï¸ Clear Chat"):
            st.session_state.messages = []
            st.rerun()


if __name__ == "__main__":
    main()