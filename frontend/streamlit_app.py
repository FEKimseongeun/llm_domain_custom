import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from pathlib import Path
import json
import re
from collections import defaultdict
from typing import List, Dict, Any
import tempfile
import os

# ì„¤ì •
st.set_page_config(
    page_title="Industrial RAG Assistant",
    page_icon="ğŸ­",
    layout="wide"
)


@st.cache_resource
def load_llm_model(model_choice: str):
    """ê°œì„ ëœ LLM ëª¨ë¸ ë¡œë“œ (CPU ìµœì í™”)"""

    # CPU ì¹œí™”ì ì¸ ëª¨ë¸ë“¤
    model_configs = {
        "deepseek-r1-distill": {
            "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
            "size": "~3GB",
            "description": "DeepSeek-R1 Distilled - ìµœê³  ì„±ëŠ¥",
            "type": "chat"
        },
        "flan-t5-small": {
            "name": "google/flan-t5-small",
            "size": "~80MB",
            "description": "Google T5 - ë¹ ë¥´ê³  íš¨ìœ¨ì ",
            "type": "text2text"
        },
        "flan-t5-base": {
            "name": "google/flan-t5-base",
            "size": "~250MB",
            "description": "Google T5 - ê· í˜•ì¡íŒ ì„±ëŠ¥",
            "type": "text2text"
        },
        "distilgpt2": {
            "name": "distilgpt2",
            "size": "~82MB",
            "description": "DistilGPT2 - ì´ˆê²½ëŸ‰",
            "type": "causal"
        },
        "gpt2": {
            "name": "gpt2",
            "size": "~124MB",
            "description": "GPT2 - í‘œì¤€ ì„±ëŠ¥",
            "type": "causal"
        },
        "microsoft/DialoGPT-small": {
            "name": "microsoft/DialoGPT-small",
            "size": "~117MB",
            "description": "ëŒ€í™” íŠ¹í™” ëª¨ë¸",
            "type": "causal"
        }
    }

    if model_choice not in model_configs:
        model_choice = "deepseek-r1-distill"

    config = model_configs[model_choice]

    try:
        with st.spinner(f"Loading {config['description']} ({config['size']})..."):
            model_name = config["name"]
            model_type = config["type"]

            # DeepSeek-R1-Distill ëª¨ë¸ (íŠ¹ë³„ ì²˜ë¦¬)
            if model_choice == "deepseek-r1-distill":
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )

                # íŒ¨ë”© í† í° ì„¤ì •
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                # ì»¤ìŠ¤í…€ ì œë„ˆë ˆì´í„° ê°ì²´ ìƒì„±
                class DeepSeekGenerator:
                    def __init__(self, model, tokenizer):
                        self.model = model
                        self.tokenizer = tokenizer

                    def generate_answer(self, question, context):
                        messages = [
                            {"role": "user",
                             "content": f"Based on the following context, please answer the question:\n\nContext: {context}\n\nQuestion: {question}"}
                        ]

                        inputs = self.tokenizer.apply_chat_template(
                            messages,
                            add_generation_prompt=True,
                            tokenize=True,
                            return_dict=True,
                            return_tensors="pt"
                        ).to(self.model.device)

                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=200,
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=self.tokenizer.eos_token_id
                        )

                        # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        response = self.tokenizer.decode(
                            outputs[0][inputs["input_ids"].shape[-1]:],
                            skip_special_tokens=True
                        )
                        return response.strip()

                generator = DeepSeekGenerator(model, tokenizer)

            # T5 ëª¨ë¸ë“¤ì€ text2text-generation ì‚¬ìš©
            elif model_type == "text2text":
                generator = pipeline(
                    "text2text-generation",
                    model=model_name,
                    device=-1,  # CPU ì‚¬ìš©
                    torch_dtype=torch.float32,
                    model_kwargs={"low_cpu_mem_usage": True}
                )
            else:
                # ê¸°íƒ€ GPT ê³„ì—´ ëª¨ë¸ë“¤
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True
                )

                # íŒ¨ë”© í† í° ì„¤ì •
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                generator = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    device=-1,  # CPU ì‚¬ìš©
                    torch_dtype=torch.float32
                )

            st.success(f"âœ… Loaded: {config['description']}")
            return generator, config

    except Exception as e:
        st.error(f"Failed to load {model_choice}: {e}")
        # í´ë°± - ê°€ì¥ ê°€ë²¼ìš´ ëª¨ë¸
        try:
            st.info("Trying fallback model...")
            generator = pipeline("text-generation", model="distilgpt2", device=-1)
            return generator, model_configs["distilgpt2"]
        except:
            return None, None


class SimpleDocumentStore:
    """ê°„ë‹¨í•œ ë¬¸ì„œ ì €ì¥ ë° ê²€ìƒ‰ (í–¥ìƒëœ ê²€ìƒ‰)"""

    def __init__(self, category: str):
        self.category = category
        self.documents = []
        self.keyword_index = defaultdict(set)
        self.phrase_index = defaultdict(set)

        # ì„¸ì…˜ ìƒíƒœì—ì„œ ë¡œë“œ
        if f"docs_{category}" in st.session_state:
            self.documents = st.session_state[f"docs_{category}"]
            self._rebuild_index()

    def _rebuild_index(self):
        """í–¥ìƒëœ ì¸ë±ì‹±"""
        self.keyword_index = defaultdict(set)
        self.phrase_index = defaultdict(set)

        for i, doc in enumerate(self.documents):
            self._index_document(doc['text'], i)

    def _index_document(self, text: str, doc_id: int):
        """ê°œì„ ëœ ë¬¸ì„œ ì¸ë±ì‹±"""
        text_lower = text.lower()

        # ë‹¨ì–´ ì¸ë±ì‹±
        words = re.findall(r'\b[a-zA-Z]{2,}\b', text_lower)
        for word in set(words):
            self.keyword_index[word].add(doc_id)

        # êµ¬ë¬¸ ì¸ë±ì‹± (2-3 ë‹¨ì–´ ì¡°í•©)
        for i in range(len(words) - 1):
            phrase = f"{words[i]} {words[i + 1]}"
            self.phrase_index[phrase].add(doc_id)

    def add_document(self, text: str, metadata: dict):
        """ë¬¸ì„œ ì¶”ê°€"""
        doc_id = len(self.documents)
        self.documents.append({
            'text': text,
            'metadata': metadata
        })

        # ì¸ë±ì‹±
        self._index_document(text, doc_id)

        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state[f"docs_{self.category}"] = self.documents

    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """í–¥ìƒëœ ê²€ìƒ‰"""
        if not self.documents:
            return []

        query_lower = query.lower()
        query_words = re.findall(r'\b[a-zA-Z]{2,}\b', query_lower)

        if not query_words:
            return []

        # ë¬¸ì„œë³„ ì ìˆ˜ ê³„ì‚°
        doc_scores = defaultdict(float)

        # ë‹¨ì–´ ë§¤ì¹­
        for word in query_words:
            for doc_id in self.keyword_index.get(word, set()):
                doc_scores[doc_id] += 1.0

        # êµ¬ë¬¸ ë§¤ì¹­ (ë” ë†’ì€ ì ìˆ˜)
        for i in range(len(query_words) - 1):
            phrase = f"{query_words[i]} {query_words[i + 1]}"
            for doc_id in self.phrase_index.get(phrase, set()):
                doc_scores[doc_id] += 2.0

        # ì •í™•í•œ êµ¬ë¬¸ ë§¤ì¹­ (ìµœê³  ì ìˆ˜)
        for doc_id, doc in enumerate(self.documents):
            if query_lower in doc['text'].lower():
                doc_scores[doc_id] += 5.0

        # ì ìˆ˜ ì •ê·œí™”
        max_score = len(query_words) + 2.0 * (len(query_words) - 1) + 5.0
        for doc_id in doc_scores:
            doc_scores[doc_id] = min(doc_scores[doc_id] / max_score, 1.0)

        # ì ìˆ˜ ìˆœ ì •ë ¬
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)

        # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        results = []
        for doc_id, score in sorted_docs[:top_k]:
            if score > 0:
                doc = self.documents[doc_id]
                results.append({
                    'text': doc['text'],
                    'metadata': doc['metadata'],
                    'score': score
                })

        return results

    def get_document_count(self) -> int:
        return len(self.documents)


def extract_text_from_file(uploaded_file) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        if uploaded_file.type == "text/plain":
            return str(uploaded_file.read(), "utf-8")
        elif uploaded_file.type == "application/pdf":
            try:
                import PyPDF2
                reader = PyPDF2.PdfReader(uploaded_file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()
                return text
            except:
                return "PDF processing not available. Please upload as text file."
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„
            return str(uploaded_file.read(), "utf-8")
    except Exception as e:
        return f"Error reading file: {e}"


def generate_answer_with_local_llm(question: str, context: str, category: str, generator, model_config) -> str:
    """ê°œì„ ëœ ë¡œì»¬ LLM ë‹µë³€ ìƒì„±"""
    if not generator:
        return "AI model not available. Please check the setup."

    # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ëª¨ë¸ì— ë”°ë¼ ì¡°ì •)
    if "deepseek" in model_config["name"].lower():
        max_context = 2000  # DeepSeekëŠ” ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì›
    elif "small" in model_config["name"]:
        max_context = 800
    else:
        max_context = 1200

    context = context[:max_context]

    try:
        # DeepSeek-R1-Distill ëª¨ë¸
        if hasattr(generator, 'generate_answer'):
            answer = generator.generate_answer(question, context)

        # T5 ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ (text2text)
        elif model_config["type"] == "text2text":
            prompt = f"Answer the question based on the context.\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:"

            result = generator(
                prompt,
                max_length=150,
                temperature=0.7,
                do_sample=True,
                early_stopping=True
            )

            answer = result[0]['generated_text'].strip()

        else:
            # GPT ê³„ì—´ ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸
            prompt = f"""Based on the following industrial documents, please answer the question:

Context: {context}

Question: {question}

Answer:"""

            result = generator(
                prompt,
                max_length=len(prompt.split()) + 100,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=generator.tokenizer.eos_token_id,
                early_stopping=True
            )

            # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            generated_text = result[0]['generated_text']
            answer = generated_text[len(prompt):].strip()

        # ë‹µë³€ ì •ì œ
        if not answer or len(answer) < 10:
            return generate_fallback_answer(question, context)

        # ë‹µë³€ ê¸¸ì´ ì œí•œ (DeepSeekëŠ” ì˜ˆì™¸)
        if not hasattr(generator, 'generate_answer'):
            sentences = answer.split('.')
            if len(sentences) > 3:
                answer = '. '.join(sentences[:3]) + '.'

        return answer

    except Exception as e:
        st.error(f"Error generating answer: {e}")
        return generate_fallback_answer(question, context)


def generate_fallback_answer(question: str, context: str) -> str:
    """í´ë°± ë‹µë³€ ìƒì„± (ê·œì¹™ ê¸°ë°˜)"""
    # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ
    sentences = context.split('.')
    relevant_sentences = []

    question_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', question.lower()))

    for sentence in sentences[:5]:  # ì²˜ìŒ 5ë¬¸ì¥ë§Œ í™•ì¸
        sentence_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', sentence.lower()))
        if question_words.intersection(sentence_words):
            relevant_sentences.append(sentence.strip())

    if relevant_sentences:
        answer = '. '.join(relevant_sentences[:2]) + '.'
        return f"Based on the documents: {answer}"
    else:
        return "I found some relevant documents, but couldn't extract a specific answer. Please try rephrasing your question or check the source documents."


def main():
    st.title("ğŸ­ Industrial Document Assistant")
    st.markdown("**Latest DeepSeek AI + Advanced document search** (CPU optimized!)")

    # ì‚¬ì´ë“œë°” - AI ëª¨ë¸ ì„¤ì •
    with st.sidebar:
        st.header("ğŸ¤– AI Model Settings")

        # ëª¨ë¸ ì„ íƒ
        model_options = {
            "deepseek-r1-distill": "ğŸš€ DeepSeek-R1 Distilled (ìµœê³ ì„±ëŠ¥)",
            "flan-t5-small": "âš¡ Google T5 Small (ë¹ ë¦„)",
            "flan-t5-base": "ğŸ¯ Google T5 Base (ê³ ì„±ëŠ¥)",
            "distilgpt2": "ğŸ’¨ DistilGPT2 (ì´ˆê²½ëŸ‰)",
            "gpt2": "ğŸ“ GPT2 (í‘œì¤€)",
            "microsoft/DialoGPT-small": "ğŸ’¬ DialoGPT (ëŒ€í™”íŠ¹í™”)"
        }

        selected_model = st.selectbox(
            "Choose AI Model:",
            options=list(model_options.keys()),
            format_func=lambda x: model_options[x],
            index=0  # DeepSeekê°€ ê¸°ë³¸ê°’
        )

        # ëª¨ë¸ ë¡œë“œ
        if st.button("Load Model") or "current_model" not in st.session_state:
            generator, model_config = load_llm_model(selected_model)
            st.session_state.current_model = selected_model
            st.session_state.generator = generator
            st.session_state.model_config = model_config

        # í˜„ì¬ ëª¨ë¸ ìƒíƒœ í‘œì‹œ
        if "generator" in st.session_state and st.session_state.generator:
            st.success(f"âœ… {st.session_state.model_config['description']}")
            st.info(f"Size: {st.session_state.model_config['size']}")
        else:
            st.warning("âŒ No model loaded")

        st.markdown("---")

        # ì¹´í…Œê³ ë¦¬ ì„ íƒ
        categories = {
            "procurement": "Procurement/Contract",
            "piping": "Piping",
            "process": "Process",
            "mechanical": "Mechanical"
        }

        selected_category = st.selectbox(
            "Document Category:",
            options=list(categories.keys()),
            format_func=lambda x: categories[x]
        )

        st.markdown("---")

        # ë¬¸ì„œ ì—…ë¡œë“œ
        st.header("ğŸ“¤ Upload Documents")
        uploaded_files = st.file_uploader(
            "Upload files",
            accept_multiple_files=True,
            type=['txt', 'md', 'pdf'],
            help="Supported: TXT, MD, PDF files"
        )

        if uploaded_files and st.button("Process Documents"):
            store = SimpleDocumentStore(selected_category)

            progress_bar = st.progress(0)
            processed = 0

            for i, uploaded_file in enumerate(uploaded_files):
                text = extract_text_from_file(uploaded_file)
                if text.strip() and len(text) > 10:
                    metadata = {
                        "category": selected_category,
                        "file_name": uploaded_file.name,
                        "file_type": uploaded_file.type
                    }
                    store.add_document(text, metadata)
                    processed += 1

                progress_bar.progress((i + 1) / len(uploaded_files))

            st.success(f"âœ… Processed {processed} documents!")
            st.rerun()

        # í†µê³„ í‘œì‹œ
        st.header("ğŸ“Š Document Stats")
        for category, name in categories.items():
            if f"docs_{category}" in st.session_state:
                count = len(st.session_state[f"docs_{category}"])
                if count > 0:
                    st.metric(name, count)

    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.header("ğŸ’¬ Chat with Documents")

    # ë„ì›€ë§
    with st.expander("â„¹ï¸ How it works", expanded=False):
        st.markdown("""
        **Enhanced Features:**

        - **Multiple AI Models**: Choose from various CPU-optimized models
        - **Improved Search**: Better keyword and phrase matching
        - **Smart Fallbacks**: Always provides useful responses
        - **No API Costs**: Runs completely free on your device

        **Model Recommendations:**
        - **ğŸš€ DeepSeek-R1 Distilled**: Latest AI, best quality (3GB)
        - **âš¡ T5-Small**: Best balance of speed and quality (80MB)
        - **ğŸ¯ T5-Base**: Higher quality but slower (250MB)
        - **ğŸ’¨ DistilGPT2**: Fastest, lightweight option (82MB)

        **Tips:**
        - Upload documents in your preferred category first
        - Ask specific questions about technical details
        - The AI provides answers based on your documents
        """)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            if message["role"] == "assistant" and "sources" in message:
                if message["sources"]:
                    with st.expander("ğŸ“„ Sources", expanded=False):
                        for i, source in enumerate(message["sources"], 1):
                            st.markdown(f"**{i}. {source['file_name']}** (Score: {source['score']:.3f})")
                            st.markdown(f"*{source['preview']}*")

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("Ask about your documents..."):
        # ë¬¸ì„œ í™•ì¸
        store = SimpleDocumentStore(selected_category)
        if store.get_document_count() == 0:
            st.error(f"No documents in {categories[selected_category]} category. Please upload documents first.")
            return

        # ëª¨ë¸ í™•ì¸
        if "generator" not in st.session_state or not st.session_state.generator:
            st.error("No AI model loaded. Please load a model first.")
            return

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Searching documents and generating answer..."):
                # ë¬¸ì„œ ê²€ìƒ‰
                search_results = store.search(prompt, top_k=3)

                if not search_results:
                    response = f"No relevant documents found in {categories[selected_category]} category. Please try different keywords or upload more documents."
                    sources = []
                else:
                    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    context_texts = []
                    sources = []

                    for result in search_results:
                        text = result['text'][:800]  # ì ì ˆí•œ ê¸¸ì´ë¡œ ì œí•œ
                        context_texts.append(text)
                        sources.append({
                            'file_name': result['metadata']['file_name'],
                            'score': result['score'],
                            'preview': text[:200] + "..." if len(text) > 200 else text
                        })

                    context = "\n\n---\n\n".join(context_texts)

                    # AIë¡œ ë‹µë³€ ìƒì„±
                    response = generate_answer_with_local_llm(
                        prompt,
                        context,
                        categories[selected_category],
                        st.session_state.generator,
                        st.session_state.model_config
                    )

                st.markdown(response)

                # ì†ŒìŠ¤ í‘œì‹œ
                if sources:
                    with st.expander("ğŸ“„ Sources", expanded=False):
                        for i, source in enumerate(sources, 1):
                            st.markdown(f"**{i}. {source['file_name']}** (Score: {source['score']:.3f})")
                            st.markdown(f"*{source['preview']}*")

                # ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "sources": sources
                })

    # ì±„íŒ… ì´ˆê¸°í™”
    if st.session_state.messages:
        col1, col2 = st.columns([1, 1])
        with col1:
            if st.button("ğŸ—‘ï¸ Clear Chat"):
                st.session_state.messages = []
                st.rerun()
        with col2:
            if st.button("ğŸ”„ Reset All"):
                for key in list(st.session_state.keys()):
                    del st.session_state[key]
                st.rerun()


if __name__ == "__main__":
    main()